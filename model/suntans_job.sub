#!/bin/bash

#SBATCH -A davis_lab          ## account to charge
#SBATCH -J suntans            ## job name
#SBATCH -p free               ## partition/queue name
#SBATCH --nodes=12              ## number of nodes the job will use
#SBATCH --ntasks=420             ## number of processes to launch
#SBATCH --cpus-per-task=1       ## number of MPI threads
                                ## total RAM request = 80 * 3 GB/core = 240 GB
#SBATCH --mail-type=end               ## send email when the job ends
#SBATCH --mail-user=shuwet1@uci.edu  ## use this email address
#SBATCH --error=slurm-%J.err ## error log file
#SBATCH --time=3-00:00:00            ## time limit for each array task
#SBATCH --constraint="mlx5_ib"  ## openmpi multiple nodes

# Run MPI application using openmpi
# load modules
module load gcc/8.4.0 
module load openmpi/4.0.3/gcc.8.4.0 hdf5/1.10.5/gcc.8.4.0-openmpi.4.0.3 netcdf-c/4.7.0/gcc.8.4.0 #anaconda

#Special case openmpi job settings                   
export UCX_TLS=rc,mm
export UCX_NET_DEVICES=mlx5_0:1
export OMP_PROC_BIND=true
export OMP_PLACES=threads
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK  # Set num threads to num of
                                             # CPU cores per MPI task.
## initalize run and copy files -------------------------

# remove old output data and clean run files
make clean
make -C ../../main clean
rm -r ./data
mkdir data
cp -r  ./rundata/* ./data
rm output*.txt

# build the code on the login node
make data

## Launching ----------------------------------------------

# make grid only
mpirun -n $SLURM_NTASKS ../../main/sun -g -vv --datadir=data > output_grid.txt

# run sun only
mpirun -n $SLURM_NTASKS ../../main/sun -s -vv --datadir=data > output_run.txt

## clean up -----------------------------------------------
rm -f ./data/*.dat.*

# move the records of the run into data

